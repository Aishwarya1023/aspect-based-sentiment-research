{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T14:01:45.243767Z",
     "start_time": "2020-09-01T14:01:39.496304Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "\n",
    "from spacy import displacy\n",
    "from textblob import TextBlob\n",
    "import aspect_based_sentiment_analysis as absa\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T14:01:45.249915Z",
     "start_time": "2020-09-01T14:01:45.245902Z"
    }
   },
   "outputs": [],
   "source": [
    "# nlp.add_pipe(nlp.create_pipe('merge_noun_chunks'))\n",
    "\n",
    "def merge_nouns(doc):\n",
    "    \"\"\"If two consecutive tokens are nouns, it concatenates them into one \\\n",
    "    representing one aspect term.\n",
    "    \n",
    "    Arguments:\n",
    "        doc {obj}: nlp document object\n",
    "    \n",
    "    Returns:\n",
    "        new_text {str}: text for retokenization after nouns have been merged\n",
    "    \"\"\"\n",
    "    \n",
    "    i=0\n",
    "    new_text = ''\n",
    "    while i<len(doc):\n",
    "        if doc[i].dep_ == 'compound':\n",
    "            compound_noun = doc[i].text + doc[i+1].text\n",
    "            new_text = new_text + ' ' + compound_noun\n",
    "            i += 2\n",
    "        else:\n",
    "            new_text = new_text + ' ' + doc[i].text\n",
    "            i += 1\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T14:06:04.490110Z",
     "start_time": "2020-09-01T14:06:04.487385Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_doc2(doc_raw):\n",
    "    new_text = merge_nouns(doc_raw)\n",
    "    doc2 = tokenize(new_text)\n",
    "    return doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T14:06:05.448975Z",
     "start_time": "2020-09-01T14:06:05.445965Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Parses the given text using nlp pipeline\n",
    "    \n",
    "    Arguments:\n",
    "        text {str}: Text to be parsed\n",
    "    \n",
    "    Returns:\n",
    "        doc {obj}: nlp document object\n",
    "    \"\"\"\n",
    "    \n",
    "    doc_raw = nlp(text)\n",
    "    return doc_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T14:06:16.670950Z",
     "start_time": "2020-09-01T14:06:16.658435Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_raw = tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T14:02:29.885369Z",
     "start_time": "2020-09-01T14:02:29.882491Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_dependencies(doc):\n",
    "    \"\"\"Plots the dependencies in the nlp document\n",
    "    \n",
    "    Arguments:\n",
    "        doc {obj}: nlp document object\n",
    "    \n",
    "    Returns: \n",
    "        displacy plot\n",
    "    \"\"\"\n",
    "    \n",
    "    displacy.serve(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T14:01:45.269512Z",
     "start_time": "2020-09-01T14:01:45.266754Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pos_tags(doc):\n",
    "    \"\"\"Prints the pos tags for each token in the document\n",
    "    \n",
    "    Arguments:\n",
    "        doc {obj}: nlp document object\n",
    "    \n",
    "    Returns:\n",
    "        token.pos_ {unicode}: POS tag of each token\n",
    "    \"\"\"\n",
    "    \n",
    "    for token in doc:\n",
    "        print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T14:01:45.274352Z",
     "start_time": "2020-09-01T14:01:45.271776Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_aspect_terms(doc):\n",
    "    \"\"\"This function returns the root noun present in noun chunks of the document.\n",
    "    \n",
    "    Arguments:\n",
    "        doc {obj}: nlp document object\n",
    "    \n",
    "    Returns:\n",
    "        aspects {list{str}}: list of root nouns as aspects\n",
    "    \"\"\"\n",
    "                                            \n",
    "    aspects = [(chunk.root.text) for chunk in doc.noun_chunks if chunk.root.pos_ == 'NOUN']\n",
    "    return aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T14:01:45.280197Z",
     "start_time": "2020-09-01T14:01:45.277127Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_sentiment_terms(doc):\n",
    "    \"\"\"This function return the adjectives and verbs that are not stopwords \\\n",
    "    or punctuations, indicating a descriptive/polarized word.\n",
    "    \n",
    "    Arguments:\n",
    "        doc {obj}: nlp document object\n",
    "    \n",
    "    Returns:\n",
    "        sentiment_terms {list{str}}: list of sentiment terms\n",
    "    \"\"\"\n",
    "    \n",
    "    sentiment_terms = []\n",
    "    if doc.is_parsed:\n",
    "        sentiment_terms.append([token.lemma_ for token in doc if \\\n",
    "                                (not token.is_stop and not token.is_punct \\\n",
    "                                 and (token.pos_ == \"ADJ\" or token.pos_ == \"VERB\"))])\n",
    "    else:\n",
    "        sentiment_terms.append('') \n",
    "    return sentiment_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T14:01:45.285164Z",
     "start_time": "2020-09-01T14:01:45.282157Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dependencies(doc):\n",
    "    \"\"\"Prints the dependency tree of the document\n",
    "    \n",
    "    Arguments:\n",
    "        doc {obj}: nlp document object\n",
    "    \n",
    "    Returns:\n",
    "        token.text {unicode}: Verbatim text content\n",
    "        token.dep_ {unicode}: Syntactic dependency relation\n",
    "        token.head.text {unicode}: The syntactic parent, or “governor”, of a token\n",
    "        token.head.pos_ {unicode}: POS tag of the governor\n",
    "        children {list}: list of children of a token\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    for token in doc:\n",
    "        print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "                [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T14:01:45.290931Z",
     "start_time": "2020-09-01T14:01:45.286949Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_opinion_pairs(doc):\n",
    "    \"\"\"This function returns the opinion pairs based on pre-defined rules.\n",
    "    \n",
    "    Arguments:\n",
    "        doc {obj}: nlp document object\n",
    "    \n",
    "    Returns:\n",
    "        opinion_pairs {list{tuple}}: list of tuples consisiting of (aspect, opinion)\n",
    "    \"\"\"\n",
    "    \n",
    "    opinion_pairs = []\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'nsubj' and TextBlob(token.head.text).polarity > 0.4:\n",
    "            opinion_pairs.append((token.text, token.head.text))\n",
    "        elif token.dep_ == 'dobj' and (token.head.pos_ == 'ADJ' or TextBlob(token.head.text).polarity > 0.4):\n",
    "            opinion_pairs.append((token.text, token.head.text))\n",
    "        elif token.dep_ == 'amod' and token.head.pos_ == 'ADJ':\n",
    "            opinion_pairs.append((token.text, token.head.text))\n",
    "    return opinion_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T14:01:45.295204Z",
     "start_time": "2020-09-01T14:01:45.292550Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_attention_gradient(text, aspects):\n",
    "    nlp = absa.load(pattern_recognizer=absa.probing.AttentionGradientProduct())\n",
    "    aspects_new = nlp(text, aspects)\n",
    "    \n",
    "    for asp in aspects_new:\n",
    "        absa.probing.explain(asp.examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T14:01:45.298233Z",
     "start_time": "2020-09-01T14:01:45.296653Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"Slow service, but the waiter were friendly. He kept us engaged in conversation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T14:04:27.059354Z",
     "start_time": "2020-09-01T14:03:52.038341Z"
    }
   },
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-67229be21b51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-4b3b23ff69c5>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdoc_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_doc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnew_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_nouns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdoc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdoc2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-4b3b23ff69c5>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdoc_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_doc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnew_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_nouns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdoc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdoc2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-4b3b23ff69c5>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdoc_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_doc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnew_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_nouns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdoc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdoc2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-4b3b23ff69c5>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdoc_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_doc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnew_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_nouns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdoc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdoc2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "... last 4 frames repeated, from the frame below ...\n",
      "\u001b[0;32m<ipython-input-23-4b3b23ff69c5>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdoc_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_doc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnew_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_nouns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdoc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdoc2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "doc = tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T14:03:37.289472Z",
     "start_time": "2020-09-01T14:03:37.280387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Slow service, but the waiter were friendly. He kept us engaged in conversation"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:55:08.418941Z",
     "start_time": "2020-09-01T13:54:30.651Z"
    }
   },
   "outputs": [],
   "source": [
    "get_aspect_terms(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:55:08.420131Z",
     "start_time": "2020-09-01T13:54:31.082Z"
    }
   },
   "outputs": [],
   "source": [
    "get_sentiment_terms(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:55:08.421562Z",
     "start_time": "2020-09-01T13:54:31.366Z"
    }
   },
   "outputs": [],
   "source": [
    "get_opinion_pairs(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
